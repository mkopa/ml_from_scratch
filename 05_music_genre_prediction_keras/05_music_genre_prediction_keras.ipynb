{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music genre prediction using Keras\n",
    "\n",
    "\n",
    "Before we improve our model we first need to learn a new tool that has some more sophisticated tools available. Therefore in this lesson we will rewrite solution used previously using [Keras](https://keras.io/).\n",
    "\n",
    "Keras is a high-level library for training neural networks. In previous lesson we have written a lot of code to get a good understanding of all the math that is involved in training of our classifier. This time we will do the same with minimal effort.\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "As usual we start with imports and definition of meta-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "IN_FILE = \"./classical_vs_rock.npz\"\n",
    "SAMPLE_WIDTH = 1200\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules imported from Keras will be explained shortly. The meta-parameters look familiar except for NUM_EPOCHS which replaces number of updates that we used before.\n",
    "\n",
    "A standard way of performing training is to use all the training data in a loop. One iteration, called an epoch, consists of going through all the training data once (preferably in random order). In previous lesson, for sake of implementation simplicity, we were selecting random audio excerpts to compose each training batch. This meant that some examples were selected multiple times while other might have not been selected at all.\n",
    "\n",
    "Now, because we are using a high-level interface, we don't have to worry about that. The default behavior of Keras is to use epochs as define before so we will go with that. This is essentially the only difference in the whole procedure compared to previous lesson and, as we shall soon see, it doesn't affect model performance (at least not in a noticeable way).\n",
    "\n",
    "The evaluation of a model is done by Keras by default after every epoch. Please not that number of epochs and evaluations will match the previous scripts (500 updates with batch size 16 is exactly 8000 - total number of training examples, e.i. one epoch).\n",
    "\n",
    "Because Keras generates training and test batches by himself, the data preparation stage is greatly simplified. We only have to load the data and prepare it in a form of 4 NumPy arrays: inputs and expected outputs (targets) for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(IN_FILE)\n",
    "x_train = data['train']\n",
    "y_train = np.zeros((x_train.shape[0]))\n",
    "y_train[int(x_train.shape[0]/2):] = 1.0\n",
    "x_test = data['test']\n",
    "y_test = np.zeros((x_test.shape[0]))\n",
    "y_test[int(x_test.shape[0]/2):] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation got simplified, and so does the model definition. Please not that everything that we were manually defining in previous lesson is not completely hidden from us, we only define type of layer, number of neurons and nonlinearity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of cost function and parameter updates also becomes trivial. We can even define a metric that we will be using during automatic evaluation of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=SGD(lr=LEARNING_RATE),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, the whole training loop also boils down to single function call that does everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.2378 - acc: 0.6931 - val_loss: 0.2163 - val_acc: 0.8050\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.1991 - acc: 0.8124 - val_loss: 0.1852 - val_acc: 0.8315\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.1745 - acc: 0.8202 - val_loss: 0.1650 - val_acc: 0.8345\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.1589 - acc: 0.8193 - val_loss: 0.1522 - val_acc: 0.8325\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.1491 - acc: 0.8196 - val_loss: 0.1441 - val_acc: 0.8335\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.1429 - acc: 0.8193 - val_loss: 0.1388 - val_acc: 0.8350\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.1386 - acc: 0.8214 - val_loss: 0.1352 - val_acc: 0.8340\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.1355 - acc: 0.8226 - val_loss: 0.1326 - val_acc: 0.8360\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.1331 - acc: 0.8244 - val_loss: 0.1309 - val_acc: 0.8315\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.1313 - acc: 0.8254 - val_loss: 0.1295 - val_acc: 0.8365\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    verbose=2,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though all parameters (weights) were hidden from us so far, we can easily access them to deploy our model somewhere else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0 parameter: 0 type: float32 shape: (1200, 256)\n",
      "Layer: 0 parameter: 1 type: float32 shape: (256,)\n",
      "Layer: 1 parameter: 0 type: float32 shape: (256, 64)\n",
      "Layer: 1 parameter: 1 type: float32 shape: (64,)\n",
      "Layer: 2 parameter: 0 type: float32 shape: (64, 1)\n",
      "Layer: 2 parameter: 1 type: float32 shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "for layer_id, layer in enumerate(model.layers):\n",
    "    weights = layer.get_weights()\n",
    "    for param_id, param in enumerate(weights):\n",
    "        print(\"Layer: {} parameter: {} type: {} shape: {}\".format(\n",
    "                layer_id, param_id, param.dtype, param.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. We have basically the same solution as before (I run both scripts multiple times and the two implementations are identical in terms of average performance) but at least we understand everything that happens under the hood. And most importantly we can now update our model with some bells and whistles we were lacing before.\n",
    "\n",
    "## Rock VS Hip-Hop\n",
    "\n",
    "At this point I also encourage you to test our solution on a much more challenging task: rock vs hip-hop (you only need to use another input file, link is available in the tutorial's readme file).\n",
    "\n",
    "You will notice that with this data, performance of current model is disappointingly low: only around 60% accuracy. In all fairness, distinguishing between classical music and rock is not that difficult because classical music rarely utilizes low frequencies and even if it does, the rhythm is usually significantly different from contemporary music's. Confronted with a real challenge our multilayer perceptron failed miserably. It's pretty clear that we should be able to do better and indeed next lesson will teach you how to do it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
