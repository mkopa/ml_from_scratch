{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving trivial problem using basic math\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "The problem is finding parameters w1 and w2 of following equation:\n",
    "```\n",
    "y = x1 * w1 + x2 * w2\n",
    "```\n",
    "\n",
    "That will fit following data (the example):\n",
    "```\n",
    "x1 = 1.0\n",
    "x2 = 0.5\n",
    "y = 20\n",
    "```\n",
    "\n",
    "Internal parameters w1 and w2 (often referred to as weights) are initialized as:\n",
    "```\n",
    "w1 = 0.2\n",
    "w2 = 0.7\n",
    "```\n",
    "\n",
    "## Problem solution - general approach\n",
    "\n",
    "In general functions we will be working with from now on represent solutions to some problems we are trying to solve. \n",
    "This particular function depends on four parameters: x1, x2, w1 and w2. X1 and x2 are input variables and y is the estimated solution to our problem (the output). In order for our equation to work properly, we need to find optimal values of \"internal\" parameters w1 and w2.\n",
    "\n",
    "We already know how to find best parameters to minimize function value. Let's use that to find optimal values of\n",
    "w1 and w2.\n",
    "\n",
    "First we need to define a function that we will be minimizing. This function will represent the error that our\n",
    "function makes. By minimizing this error function (also called cost function or loss function), we will make the \n",
    "original function solve the task we want it to solve. If we manage to minimize loss to zero, our function will give perfect answers.\n",
    "\n",
    "Typical loss function looks like this:\n",
    "```\n",
    "loss = (y - expected_value)^2\n",
    "```\n",
    "\n",
    "We will optimize the cost function using gradient descent algorithm described in previous lesson.\n",
    "Please note that the cost function depends on four variables: x1, x2, w1, w2 and expected_value (often called target value). The \"x\" variables paired with expected_value are examples which we will be using for training our model. During the optimization process the \"x\" variables and expected_value will be fixed and we will be optimizing \"w\" variables to minimize cost function. This notation might be a bit confusing because in previous lesson we were finding optimal values of \"x\" variable, but now \"x\" represents input which is given by the example and therefore it is fixed and only internal parameters \"w\" are subject of optimization.\n",
    "\n",
    "## Problem solution - implementation\n",
    "\n",
    "We start by defining two meta-parameter of our algorithm: learning rate and number of updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.2\n",
    "NUM_UPDATES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate in the portion of gradient value we will subtracting from current values of parameters we are optimizing (w1 and w2). In other words this is step size we used in first lesson. In this example it is relatively high because our problem is trivial and we want the model to train fast. In most real-life cases we use smaller learning rates.\n",
    "\n",
    "Number of updates is simply the fixed number of iterations we will perform our optimization procedure.\n",
    "\n",
    "Now let's define the function  and the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x1, x2, w1, w2):\n",
    "    return x1*w1 + x2*w2\n",
    "\n",
    "def cost_function(x1, x2, w1, w2, target):\n",
    "    diff = function(x1, x2, w1, w2) - target\n",
    "    return diff * diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define gradient function. Because our problem is trivial we can write our own gradient function that will be an approximation of real gradient. To do this we use the very definition of a derivative but instead of using infinitely small increment or parameter value we will use small constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_gradient(x1, x2, w1, w2, t):\n",
    "    eps = 1e-6\n",
    "\n",
    "    dcost_w1 = cost_function(x1, x2, w1 + eps, w2, t) - cost_function(x1, x2, w1 - eps, w2, t)\n",
    "    dcost_w2 = cost_function(x1, x2, w1, w2 + eps, t) - cost_function(x1, x2, w1, w2 - eps, t)\n",
    "\n",
    "    return dcost_w1/(eps*2), dcost_w2/(eps*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the function returns gradient for each parameter separately. For each parameter, we first evaluate the cost function with slightly increased parameter value and then with slightly decreased parameter value. We than calculate difference between these two function values and divide it by doubled epsilon value (the total change in the parameter value).\n",
    "\n",
    "We can now proceed with the declaration of our variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: 0.55\n"
     ]
    }
   ],
   "source": [
    "w1 = 0.2\n",
    "w2 = 0.7\n",
    "\n",
    "x1 = 1.0\n",
    "x2 = 0.5\n",
    "target = 20\n",
    "\n",
    "print \"Initial output: {}\".format(function(x1, x2, w1, w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our trivial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after update 1 is 10.2750000124\n",
      "Output after update 2 is 15.1375000065\n",
      "Output after update 3 is 17.5687500025\n",
      "Output after update 4 is 18.7843750008\n",
      "Output after update 5 is 19.3921875002\n",
      "Output after update 6 is 19.69609375\n",
      "Output after update 7 is 19.848046875\n",
      "Output after update 8 is 19.9240234375\n",
      "Output after update 9 is 19.9620117187\n",
      "Output after update 10 is 19.9810058594\n",
      "Final weights values: w1: 15.7448046874 w2: 8.47240234396\n"
     ]
    }
   ],
   "source": [
    "for update in range(NUM_UPDATES):\n",
    "    grad_w1, grad_w2 = cost_function_gradient(x1, x2, w1, w2, target)\n",
    "    w1 -= LEARNING_RATE * grad_w1\n",
    "    w2 -= LEARNING_RATE * grad_w2\n",
    "    print \"Output after update {} is {}\".format(update + 1, function(x1, x2, w1, w2))\n",
    "print \"Final weights values: w1: {} w2: {}\".format(w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we gradually got closer and closer to our expected value (the target). After 10 updates we already quite close to expected value of 20.0.\n",
    "\n",
    "Because learning rate was quite small we only approached the target value from one side. Now let's see what will happen if we increase learning rate to 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after update 1 is 29.7250000372\n",
      "Output after update 2 is 15.1374999674\n",
      "Output after update 3 is 22.4312500124\n",
      "Output after update 4 is 18.7843749901\n",
      "Output after update 5 is 20.6078125042\n",
      "Output after update 6 is 19.6960937469\n",
      "Output after update 7 is 20.1519531267\n",
      "Output after update 8 is 19.9240234369\n",
      "Output after update 9 is 20.0379882814\n",
      "Output after update 10 is 19.9810058593\n",
      "Final weights values: w1: 15.7448046836 w2: 8.47240235136\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.6\n",
    "\n",
    "w1 = 0.2\n",
    "w2 = 0.7\n",
    "\n",
    "for update in range(NUM_UPDATES):\n",
    "    grad_w1, grad_w2 = cost_function_gradient(x1, x2, w1, w2, target)\n",
    "    w1 -= LEARNING_RATE * grad_w1\n",
    "    w2 -= LEARNING_RATE * grad_w2\n",
    "    print \"Output after update {} is {}\".format(update + 1, function(x1, x2, w1, w2))\n",
    "print \"Final weights values: w1: {} w2: {}\".format(w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is nearly identical, but as you can see we jumped between values that were greater or smaller than our target value. This case is much more like the one described in first lesson where the derivative was changing sign after every update. If we increase learning rate too high the whole procedure will become unstable and all values will start growing instead of converging to optimal solution.\n",
    "\n",
    "This is all for lesson one. I encourage you to experiment with different learning rates while observing values of gradients and weights. We will soon use this technique on a much greater scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
