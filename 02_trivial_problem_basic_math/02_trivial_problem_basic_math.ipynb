{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving trivial problem using basic math\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "The problem is finding parameters w1 and w2 of following equation:\n",
    "```\n",
    "y = x1 * w1 + x2 * w2\n",
    "```\n",
    "\n",
    "That will fit following data (the example):\n",
    "```\n",
    "x1 = 1.0\n",
    "x2 = 0.5\n",
    "y = 20\n",
    "```\n",
    "\n",
    "Weights are initialized as:\n",
    "```\n",
    "w1 = 0.2\n",
    "w2 = 0.7\n",
    "```\n",
    "\n",
    "## Problem solution - theory\n",
    "\n",
    "In general functions we will be working with from now on represent solutions to some problems we are trying to solve. \n",
    "This particular function depends on four parameters: x1, x2, w1 and w2. X1 and x2 are input variables and y is the estimated solution to our problem (the output). In order for our equation to work properly, we need to find optimal values of \"internal\" parameters w1 and w2.\n",
    "\n",
    "We already know how to find best parameters to minimize function value. Let's use that to find optimal values of\n",
    "w1 and w2.\n",
    "\n",
    "First we need to define a function that we will be minimizing. This function will represent the error that our\n",
    "function makes. By minimizing this error function (malso called cost function or loss function), we will make the \n",
    "original function solve the task we want it to solve. If we manage to minimize loss to zero, our function will give perfect answers.\n",
    "\n",
    "Typical loss function looks like this:\n",
    "```\n",
    "loss = (y - expected_value)^2\n",
    "```\n",
    "\n",
    "We will optimize the cost function using gradient descent algorithm described in previous lesson.\n",
    "Please note that the cost function depends on four variables: x1, x2, w1, w2 and expected_value (often called target value). The \"x\" variables paired with expected_value are examples which we will be using for training our model. During the optimization process the \"x\" variables and expected_value will be fixed and we will be optimizing \"w\" variables to minimize cost function. This notation might be a bit confusing because in previous lesson we were finding optimal values of \"x\" variable, but now \"x\" represents input which is given by the example and therefore it is fixed and only internal parameters \"w\" are subject of optimization.\n",
    "\n",
    "## Problem solution - implementation\n",
    "\n",
    "We start by defining two meta-parameter of our algorithm: learning rate and number of updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "NUM_UPDATES = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate in the portion of gradient value we will subtracting from current values of parameters we are optimizing (w1 and w2). In other words this is step size we used in first lesson. In this example it is relatively high because our problem is trivial and we want the model to train fast. In most real-life cases we use smaller learning rates.\n",
    "\n",
    "Number of updates is simply the fixed number of iterations we will perform our optimization procedure.\n",
    "\n",
    "Now let's define the function  and the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x1, x2, w1, w2):\n",
    "    return x1*w1 + x2*w2\n",
    "\n",
    "def cost_function(x1, x2, w1, w2, target):\n",
    "    diff = function(x1, x2, w1, w2) - target\n",
    "    return diff * diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define gradient function. Because our problem is trivial we can write our own gradient function that will be an approximation of real gradient. To do this we use the very definition of a derivative but instead of using inifinitely small increment or parameter value we will use small constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_gradient(x1, x2, w1, w2, t):\n",
    "    eps = 1e-6\n",
    "\n",
    "    dcost_w1 = cost_function(x1, x2, w1 + eps, w2, t) - cost_function(x1, x2, w1 - eps, w2, t)\n",
    "    dcost_w2 = cost_function(x1, x2, w1, w2 + eps, t) - cost_function(x1, x2, w1, w2 - eps, t)\n",
    "\n",
    "    return dcost_w1/(eps*2), dcost_w2/(eps*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the function returns gradient for each parameter separately. For each parameter, we first evalueate the cost function with slightly increased parameter value and then with slightly decreased parameter value. We than calculate difference between these two function values and divide it by doubled epsilon value (the total change in the parameter value).\n",
    "\n",
    "We can now proceed with the declaration of our variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial output: 0.55\n"
     ]
    }
   ],
   "source": [
    "w1 = 0.2\n",
    "w2 = 0.7\n",
    "\n",
    "x1 = 1.0\n",
    "x2 = 0.5\n",
    "target = 20\n",
    "\n",
    "print \"Initial output: {}\".format(function(x1, x2, w1, w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our trivial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after update 1 is 5.4125000062\n",
      "Output after update 2 is 9.05937500445\n",
      "Output after update 3 is 11.7945312539\n",
      "Output after update 4 is 13.8458984395\n",
      "Output after update 5 is 15.3844238289\n",
      "Output after update 6 is 16.5383178712\n",
      "Output after update 7 is 17.403738403\n",
      "Output after update 8 is 18.052803802\n",
      "Output after update 9 is 18.5396028513\n",
      "Output after update 10 is 18.9047021383\n",
      "Output after update 11 is 19.1785266037\n",
      "Output after update 12 is 19.3838949522\n",
      "Output after update 13 is 19.5379212138\n",
      "Output after update 14 is 19.65344091\n",
      "Output after update 15 is 19.7400806825\n",
      "Output after update 16 is 19.8050605119\n",
      "Output after update 17 is 19.8537953839\n",
      "Output after update 18 is 19.8903465379\n",
      "Output after update 19 is 19.9177599034\n",
      "Output after update 20 is 19.9383199275\n",
      "Final weights values: w1: 15.710655941 w2: 8.45532797306\n"
     ]
    }
   ],
   "source": [
    "for update in range(NUM_UPDATES):\n",
    "    grad_w1, grad_w2 = cost_function_gradient(x1, x2, w1, w2, target)\n",
    "    w1 -= LEARNING_RATE * grad_w1\n",
    "    w2 -= LEARNING_RATE * grad_w2\n",
    "    print \"Output after update {} is {}\".format(update + 1, function(x1, x2, w1, w2))\n",
    "print \"Final weights values: w1: {} w2: {}\".format(w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we gradually got closer and closer to our expected value (the target). After 20 updates we are at 19.94 which is already quite close to expected value of 20.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
